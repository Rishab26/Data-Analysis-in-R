{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-based propaganda detection\n",
    "Propaganda is the new weapon that influences people's opinions or beliefs about a certain ideology, whether that ideology is right or wrong. This assignment requires you to design a propaganda content identifier. Below presents the sample code for using the provided dataset to train an MLP-based propaganda detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>730865684</td>\n",
       "      <td>Puerto Rico Hurricane Recovery Worsened By Nea...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>After 2011, the territory adopted a uniform bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>730268758</td>\n",
       "      <td>Evidence shows Pope Francis is a ‘principal in...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Reports say Pope Francis personally received t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>790677230</td>\n",
       "      <td>Kavanaugh's Nomination Saved?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Attorneys for new Kavanaugh accusers Deborah R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>703821117</td>\n",
       "      <td>The Cunning CIA</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Shapira points out, “Several news organization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7053</th>\n",
       "      <td>703821117</td>\n",
       "      <td>The Cunning CIA</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>Interesting enough, in the Chile regime-change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5256</th>\n",
       "      <td>728972961</td>\n",
       "      <td>FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>I was in Melbourne, Australia, during the 40th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7848</th>\n",
       "      <td>790677230</td>\n",
       "      <td>Kavanaugh's Nomination Saved?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>“Democratic staff was invited to participate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>703821117</td>\n",
       "      <td>The Cunning CIA</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>There could be only one answer: communists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>730268758</td>\n",
       "      <td>Evidence shows Pope Francis is a ‘principal in...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>“This is Barros present, in the room when the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>774145019</td>\n",
       "      <td>Obama Was Never As Tough On Russia As Trump</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>We are doing NATO exercises in the Baltics.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id                                      article_title  \\\n",
       "4309   730865684  Puerto Rico Hurricane Recovery Worsened By Nea...   \n",
       "3841   730268758  Evidence shows Pope Francis is a ‘principal in...   \n",
       "7857   790677230                      Kavanaugh's Nomination Saved?   \n",
       "7031   703821117                                    The Cunning CIA   \n",
       "7053   703821117                                    The Cunning CIA   \n",
       "...          ...                                                ...   \n",
       "5256   728972961  FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...   \n",
       "7848   790677230                      Kavanaugh's Nomination Saved?   \n",
       "6999   703821117                                    The Cunning CIA   \n",
       "3865   730268758  Evidence shows Pope Francis is a ‘principal in...   \n",
       "7391   774145019        Obama Was Never As Tough On Russia As Trump   \n",
       "\n",
       "               label                                      sentence_text  \n",
       "4309  non-propaganda  After 2011, the territory adopted a uniform bu...  \n",
       "3841  non-propaganda  Reports say Pope Francis personally received t...  \n",
       "7857  non-propaganda  Attorneys for new Kavanaugh accusers Deborah R...  \n",
       "7031  non-propaganda  Shapira points out, “Several news organization...  \n",
       "7053      propaganda  Interesting enough, in the Chile regime-change...  \n",
       "...              ...                                                ...  \n",
       "5256  non-propaganda  I was in Melbourne, Australia, during the 40th...  \n",
       "7848  non-propaganda  “Democratic staff was invited to participate a...  \n",
       "6999      propaganda        There could be only one answer: communists.  \n",
       "3865  non-propaganda  “This is Barros present, in the room when the ...  \n",
       "7391  non-propaganda        We are doing NATO exercises in the Baltics.  \n",
       "\n",
       "[11464 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.read_table('coursework2_train.tsv')\n",
    "df = shuffle(df) # randomly shuffle data entries \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size: 11464, label type num: 2\n"
     ]
    }
   ],
   "source": [
    "raw_labels = df.label.values.tolist()\n",
    "docs = df.sentence_text.values.tolist()\n",
    "titles = df.article_title.values.tolist()\n",
    "\n",
    "label_dic = {'non-propaganda':0, 'propaganda':1}\n",
    "\n",
    "assert len(docs) == len(raw_labels) == len(titles)\n",
    "labels = [label_dic[rl] for rl in raw_labels] # transfer raw labels (strings) to integer numbers\n",
    "print('total data size: {}, label type num: {}'.format(len(docs), len(label_dic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trumpism is not a detour, after which we can all get back on the interstate to the New World Order.\n",
      "Patrick J. Buchanan: Sorry, Jeff Flake, It's Trump's Party Now!\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# take a look at some sentences in the dataset\n",
    "print(docs[19])\n",
    "print(titles[19])\n",
    "print(labels[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'D:/data/text/data_edited.csv', header=None, index=None, sep='\\t', mode='a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of non-propoganda entries 8227\n",
      "num of propoganda entries 3237\n"
     ]
    }
   ],
   "source": [
    "print('num of non-propoganda entries', len([l for l in labels if l== 0]))\n",
    "print('num of propoganda entries', len([l for l in labels if l== 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 6878, dev size 2293, test size 2292\n"
     ]
    }
   ],
   "source": [
    "# split the data into train, dev and test\n",
    "\n",
    "train_ratio, dev_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "train_docs = docs[:int(len(docs)*train_ratio)]\n",
    "train_labels = labels[:int(len(docs)*train_ratio)]\n",
    "\n",
    "dev_docs = docs[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "dev_labels = labels[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "\n",
    "test_docs = docs[-int(len(docs)*(test_ratio)):]\n",
    "test_labels = labels[-int(len(docs)*(test_ratio)):]\n",
    "\n",
    "print('train size {}, dev size {}, test size {}'.format(len(train_labels), len(dev_labels), len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#for i in range(1,35000,1000):\n",
    "#max_feature_num = 1000\n",
    "def logistic(train_data,test_data,train_labels,test_labels):\n",
    "    train_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "                encoding='utf-8',input='content', lowercase=True, max_df=20000,\n",
    "                max_features=40000, min_df=1, ngram_range=(1, 2), norm='l2',\n",
    "                preprocessor=None, smooth_idf=True, stop_words=None,\n",
    "                strip_accents=None, sublinear_tf=False,\n",
    "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True)\n",
    "    train_vecs = train_vectorizer.fit_transform(train_data)\n",
    "    test_vecs = TfidfVectorizer(vocabulary=train_vectorizer.vocabulary_).fit_transform(test_data)\n",
    "\n",
    "# train model\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(C=10000.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
    "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
    "                   warm_start=False).fit(train_vecs, train_labels)\n",
    "\n",
    "# test model\n",
    "    test_pred = clf.predict(test_vecs)\n",
    "    from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "    acc = accuracy_score(test_labels, test_pred)\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "    # print(max_feature_num)\n",
    "    print('acc', acc)\n",
    "    print('precision', pre)\n",
    "    print('rec', rec)\n",
    "    print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def word_removal(data):\n",
    "    for i in range(0,len(data)):\n",
    "        for word in data[i]:\n",
    "            if word in string.punctuation: # remove all punctuations\n",
    "                data[i].remove(word)\n",
    "            elif word in stop_words:\n",
    "                data[i].remove(word)\n",
    "            elif (word.isnumeric() == True):\n",
    "                data[i].remove(word)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building different sets of data\n",
    "all_text = pd.DataFrame()\n",
    "all_text['text'] = df['sentence_text']\n",
    "all_lables = df['label'].tolist()\n",
    "# Set 1 : lemmatized data\n",
    "# Set 2 : Stemmed data\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps_stemmer = PorterStemmer()\n",
    "all_text['text_tokenized']=all_text['text'].apply(nltk.word_tokenize)\n",
    "all_text['text_lemmatized']=all_text['text'].apply(nltk.word_tokenize).apply(lambda row: list(wordnet_lemmatizer.lemmatize(row[row.index(y)].lower()) for y in row))\n",
    "all_text['text_stemmed']=all_text['text'].apply(nltk.word_tokenize).apply(lambda row: list(ps_stemmer.stem(row[row.index(y)].lower()) for y in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text['text_tokenized_stopwordspunctnum_removal'] = word_removal(all_text['text_tokenized'])\n",
    "all_text['text_lemmatized_stopwordspunctnum_removal'] = word_removal(all_text['text_lemmatized'])\n",
    "all_text['text_stemmed_stopwordspunctnum_removal'] = word_removal(all_text['text_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_tokenized =[\" \".join(review) for review in all_text['text_tokenized'][:int(len(docs)*0.8)].values]\n",
    "train_text_lemmatized =[\" \".join(review) for review in all_text['text_lemmatized'][:int(len(docs)*0.8)].values]\n",
    "train_text_stemmed =[\" \".join(review) for review in all_text['text_stemmed'][:int(len(docs)*0.8)].values]\n",
    "train_text_tokenized_stopwordspunctnum_removal =[\" \".join(review) for review in all_text['text_tokenized_stopwordspunctnum_removal'][:int(len(docs)*0.8)].values]\n",
    "train_text_lemmatized_stopwordspunctnum_removal =[\" \".join(review) for review in all_text['text_lemmatized_stopwordspunctnum_removal'][:int(len(docs)*0.8)].values]\n",
    "train_text_stemmed_stopwordspunctnum_removal =[\" \".join(review) for review in all_text['text_stemmed_stopwordspunctnum_removal'][:int(len(docs)*0.8)].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2292"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels[:int(len(docs)*(0.8)):]\n",
    "test_text = docs[-int(len(docs)*(test_ratio)):]\n",
    "test_labels = labels[-int(len(docs)*(test_ratio)):]\n",
    "# train_labels = all_lables[:35000]\n",
    "# test_text = [\" \".join(review) for review in all_text['text'][35000:].values]\n",
    "# test_labels = all_lables[35000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_text_tokenized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7185863874345549\n",
      "precision 0.6375897393701273\n",
      "rec 0.628914098705684\n",
      "f1 0.6325629451633117\n",
      "train_text_lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7046247818499127\n",
      "precision 0.6247899651078926\n",
      "rec 0.6233864416285773\n",
      "f1 0.6240659394705277\n",
      "train_text_stemmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6631762652705061\n",
      "precision 0.5952572754166929\n",
      "rec 0.6060908305073476\n",
      "f1 0.5981486107767567\n",
      "train_text_tokenized_stopwordspunctnum_removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7203315881326352\n",
      "precision 0.6407634941011064\n",
      "rec 0.6331465725345985\n",
      "f1 0.636439224035854\n",
      "train_text_lemmatized_stopwordspunctnum_removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6958987783595113\n",
      "precision 0.6212254690114469\n",
      "rec 0.6265087583876126\n",
      "f1 0.6235130757423751\n",
      "train_text_stemmed_stopwordspunctnum_removal\n",
      "acc 0.6762652705061082\n",
      "precision 0.5953776020376224\n",
      "rec 0.5983717696308439\n",
      "f1 0.5967066166461937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_text_tokenized\")\n",
    "logistic(train_text_tokenized,test_text,train_labels,test_labels)\n",
    "print(\"train_text_lemmatized\")\n",
    "logistic(train_text_lemmatized,test_text,train_labels,test_labels)\n",
    "print(\"train_text_stemmed\")\n",
    "logistic(train_text_stemmed,test_text,train_labels,test_labels)\n",
    "print(\"train_text_tokenized_stopwordspunctnum_removal\")\n",
    "logistic(train_text_tokenized_stopwordspunctnum_removal,test_text,train_labels,test_labels)\n",
    "print(\"train_text_lemmatized_stopwordspunctnum_removal\")\n",
    "logistic(train_text_lemmatized_stopwordspunctnum_removal,test_text,train_labels,test_labels)\n",
    "print(\"train_text_stemmed_stopwordspunctnum_removal\")\n",
    "logistic(train_text_stemmed_stopwordspunctnum_removal,test_text,train_labels,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the glove pre-trained embedding\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "path_of_downloaded_files = \"D:/Downloads/glove.6B/glove.6B.300d.txt\"\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Out-of-vocabulary (OOV) words: words that are not included in the pre-trained embedding model\n",
    "# There exist many ways to vectorize OOV words, e.g. use a random vector to represent all OOV words\n",
    "# Feel free to search and employ other ways to vectorize OOV words\n",
    "word_vec_dim = 300 # make sure this number matches the embedding you use\n",
    "oov_vec = np.random.rand(word_vec_dim) \n",
    "def vectorize_sent(word_vectors, sent):\n",
    "    word_vecs = []\n",
    "    for token in word_tokenize(sent): \n",
    "        if token not in word_vectors: \n",
    "            word_vecs.append(oov_vec)\n",
    "        else:\n",
    "            word_vecs.append(word_vectors[token].astype('float64'))\n",
    "    return np.mean(word_vecs,axis=0)\n",
    "\n",
    "vv = vectorize_sent(word_vectors, 'hello world ! this is a test sentence !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878, 300)\n"
     ]
    }
   ],
   "source": [
    "# create vector representations; \n",
    "# TODO: consider to apply necessary text cleaning/normalization techniques\n",
    "# TODO: consider whether to use titles information (the example below does not use titles but only sentences)\n",
    "\n",
    "train_vecs = np.array([vectorize_sent(word_vectors, ss) for ss in train_docs])\n",
    "dev_vecs = np.array([vectorize_sent(word_vectors, ss) for ss in dev_docs])\n",
    "print(train_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple MLP (multi-layer perceptron) as the classifation model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dp_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, input_dim*2)\n",
    "        self.output_layer = nn.Linear(input_dim*2, out_dim)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "       \n",
    "    def forward(self, x_in):\n",
    "        z1 = self.dropout(x_in) # output of the input layer, after dropout\n",
    "        z2 = self.relu(self.hidden_layer(z1)) # output of the hidden layer\n",
    "        logits = self.output_layer(z2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "dropout_rate = 0.5 \n",
    "model = MLP(word_vec_dim,len(label_dic),dropout_rate) \n",
    "loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 50 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 32 # mini batch size\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# initialize optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9) # decays the learning rate of each parameter group by gamma every step_size epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-f1 on dev set is 0.41890522047643186\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.41890522047643186\n",
      "\n",
      "---> after epoch 1 the macro-f1 on dev set is 0.46072138778872235\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.46072138778872235\n",
      "\n",
      "---> after epoch 2 the macro-f1 on dev set is 0.48078330190388874\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.48078330190388874\n",
      "\n",
      "---> after epoch 3 the macro-f1 on dev set is 0.46893978383942353\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 4 the macro-f1 on dev set is 0.4967415175678973\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.4967415175678973\n",
      "\n",
      "---> after epoch 5 the macro-f1 on dev set is 0.545107263023039\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.545107263023039\n",
      "\n",
      "---> after epoch 6 the macro-f1 on dev set is 0.5077161347612124\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 7 the macro-f1 on dev set is 0.5348631251886264\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 8 the macro-f1 on dev set is 0.5661440379784787\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.5661440379784787\n",
      "\n",
      "---> after epoch 9 the macro-f1 on dev set is 0.541766138573127\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 10 the macro-f1 on dev set is 0.5605019574481092\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 11 the macro-f1 on dev set is 0.5931246884727563\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.5931246884727563\n",
      "\n",
      "---> after epoch 12 the macro-f1 on dev set is 0.571027335721055\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 13 the macro-f1 on dev set is 0.5692163177670424\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 14 the macro-f1 on dev set is 0.5883015767301623\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 15 the macro-f1 on dev set is 0.5832354805151323\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 16 the macro-f1 on dev set is 0.5776823883392891\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 17 the macro-f1 on dev set is 0.6049024109716046\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.6049024109716046\n",
      "\n",
      "---> after epoch 18 the macro-f1 on dev set is 0.6082188592706961\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.6082188592706961\n",
      "\n",
      "---> after epoch 19 the macro-f1 on dev set is 0.590745213759623\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 20 the macro-f1 on dev set is 0.5687736192377821\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 21 the macro-f1 on dev set is 0.5998310669310027\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 22 the macro-f1 on dev set is 0.5882740856440496\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 23 the macro-f1 on dev set is 0.5829003874705452\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 24 the macro-f1 on dev set is 0.6078910285204917\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 25 the macro-f1 on dev set is 0.5966676573522008\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 26 the macro-f1 on dev set is 0.5762256381171211\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 27 the macro-f1 on dev set is 0.5946107013230301\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 28 the macro-f1 on dev set is 0.6089545807569993\n",
      "learning rate 0.0008100000000000001\n",
      "best model updated; new best f1 0.6089545807569993\n",
      "\n",
      "---> after epoch 29 the macro-f1 on dev set is 0.5771839367902535\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 30 the macro-f1 on dev set is 0.5999356904837777\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 31 the macro-f1 on dev set is 0.5972096082762436\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 32 the macro-f1 on dev set is 0.61219925701429\n",
      "learning rate 0.0007290000000000002\n",
      "best model updated; new best f1 0.61219925701429\n",
      "\n",
      "---> after epoch 33 the macro-f1 on dev set is 0.60360469123375\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 34 the macro-f1 on dev set is 0.6066836704253923\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 35 the macro-f1 on dev set is 0.6005144602266904\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 36 the macro-f1 on dev set is 0.6034853783191791\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 37 the macro-f1 on dev set is 0.5930128544193082\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 38 the macro-f1 on dev set is 0.6096055931687988\n",
      "learning rate 0.0007290000000000002\n",
      "\n",
      "---> after epoch 39 the macro-f1 on dev set is 0.6225380389524037\n",
      "learning rate 0.0007290000000000002\n",
      "best model updated; new best f1 0.6225380389524037\n",
      "\n",
      "---> after epoch 40 the macro-f1 on dev set is 0.6374007569234805\n",
      "learning rate 0.0006561000000000001\n",
      "best model updated; new best f1 0.6374007569234805\n",
      "\n",
      "---> after epoch 41 the macro-f1 on dev set is 0.614545985388544\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 42 the macro-f1 on dev set is 0.6178151929541423\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 43 the macro-f1 on dev set is 0.6072829481430906\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 44 the macro-f1 on dev set is 0.6190369920339697\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 45 the macro-f1 on dev set is 0.6336683620081913\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 46 the macro-f1 on dev set is 0.6172466126804027\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 47 the macro-f1 on dev set is 0.650541410623922\n",
      "learning rate 0.0006561000000000001\n",
      "best model updated; new best f1 0.650541410623922\n",
      "\n",
      "---> after epoch 48 the macro-f1 on dev set is 0.6211864302299299\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 49 the macro-f1 on dev set is 0.6285748787195056\n",
      "learning rate 0.0006561000000000001\n"
     ]
    }
   ],
   "source": [
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    for idx in range(0,len(train_vecs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        x_data = torch.tensor(train_vecs[idx:idx+batch_size], dtype=torch.float)\n",
    "        if x_data.shape[0] == 0: continue\n",
    "        y_target = torch.tensor(train_labels[idx:idx+batch_size], dtype=torch.int64)\n",
    "\n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = model(x_data)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        dev_data = torch.tensor(dev_vecs, dtype=torch.float)\n",
    "        dev_target = torch.tensor(dev_labels, dtype=torch.int64)\n",
    "        dev_prediction = model(dev_data)\n",
    "        pred_labels = [np.argmax(dp.numpy()) for dp in dev_prediction]\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(dev_target, pred_labels, average='macro')\n",
    "        print('\\n---> after epoch {} the macro-f1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best f1',f1)\n",
    "            \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-f1 on test data 0.6538237226426551\n"
     ]
    }
   ],
   "source": [
    "# test on the test set\n",
    "\n",
    "# load the best model weights\n",
    "model.load_state_dict(best_model) \n",
    "test_vecs = np.array([vectorize_sent(word_vectors, ss) for ss in test_docs])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "    test_data = torch.tensor(test_vecs, dtype=torch.float)\n",
    "    test_target = torch.tensor(test_labels, dtype=torch.int64)\n",
    "    test_prediction = model(test_data)\n",
    "    pred_labels = [np.argmax(dp.numpy()) for dp in test_prediction]\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(test_target, pred_labels, average='macro')\n",
    "    print('macro-f1 on test data', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 6878, dev size 2293, test size 2292\n"
     ]
    }
   ],
   "source": [
    "train_ratio, dev_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "train_docs = docs[:int(len(docs)*train_ratio)]\n",
    "train_labels = labels[:int(len(docs)*train_ratio)]\n",
    "\n",
    "dev_docs = docs[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "dev_labels = labels[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "\n",
    "test_docs = docs[-int(len(docs)*(test_ratio)):]\n",
    "test_labels = labels[-int(len(docs)*(test_ratio)):]\n",
    "\n",
    "print('train size {}, dev size {}, test size {}'.format(len(train_labels), len(dev_labels), len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = ['pos','neg']\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, model_type, cls_num, pooler_type, dropout, gpu):\n",
    "        super(RNN_Classifier, self).__init__()\n",
    "        assert model_type in ['rnn','lstm','bilstm','gru']\n",
    "        assert pooler_type in ['max','avg']\n",
    "        # rnn type\n",
    "        if model_type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'bilstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, bidirectional=True, dropout=dropout)\n",
    "        else: # model_type == 'gru'\n",
    "            self.rnn = nn.GRU(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        # map from rnn output to logits\n",
    "        if model_type == 'bilstm':\n",
    "            self.fc = nn.Linear(2*hidden_dim, cls_num)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, cls_num)\n",
    "        # pooler type\n",
    "        self.pooler_type = pooler_type\n",
    "        # gpu or not\n",
    "        self.gpu = gpu\n",
    "        if gpu: self.to('cuda')\n",
    "            \n",
    "    def forward(self, input_matrix):\n",
    "        token_num = input_matrix.shape[1]\n",
    "        hidden_vecs = self.rnn(input_matrix)[0]\n",
    "        if self.pooler_type == 'max':\n",
    "            pooler = nn.MaxPool1d(token_num)\n",
    "        else: \n",
    "            pooler = nn.AvgPool1d(token_num)\n",
    "        if self.gpu: pooler.to('cuda')\n",
    "        pooled_hidden = pooler(torch.transpose(hidden_vecs,1,2)).squeeze()\n",
    "        return self.fc(pooled_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0353, -0.0811],\n",
      "        [ 0.0526, -0.0938],\n",
      "        [-0.0293, -0.0376]], device='cuda:0', grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "labels_list= ['non-propaganda', 'propaganda']\n",
    "embd_dim = 300\n",
    "hidden_dim = 300\n",
    "rnn_type = 'bilstm'\n",
    "pooler_type = 'avg'\n",
    "dropout = 0.5\n",
    "gpu = True\n",
    "\n",
    "oov_vec = oov_vec = np.random.rand(embd_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    return np.array(vecs)\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors):\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def make_batch_prediction(sent_list, word_vectors, model, use_gpu=True):\n",
    "    batch = build_mini_batch(sent_list, word_vectors)\n",
    "    batch_logits = torch.tensor([])\n",
    "    if use_gpu: batch_logits = batch_logits.to('cuda')\n",
    "    for i in range(batch.shape[0]):\n",
    "        input_sents = torch.from_numpy(batch[i]).float()\n",
    "        if use_gpu: input_sents = input_sents.to('cuda')\n",
    "        logits = model(input_sents.unsqueeze(0))\n",
    "        batch_logits = torch.cat( (batch_logits, logits) )\n",
    "    return batch_logits.view(batch.shape[0],-1)\n",
    "  \n",
    "# sanity check \n",
    "model = RNN_Classifier(embd_dim, hidden_dim, rnn_type, len(labels_list), pooler_type, dropout, gpu)\n",
    "batch_pred = make_batch_prediction(\n",
    "    ['hello world!','hello','another test sentence this is'],\n",
    "    word_vectors, model, gpu)\n",
    "print(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 50 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 50\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.999) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======epoch 0 loss====== 0.06530766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:43<35:44, 43.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-F1 on dev set is 0.6790821538489711\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.6790821538489711\n",
      "\n",
      "======epoch 1 loss====== 0.07991626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [01:27<34:59, 43.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 1 the macro-F1 on dev set is 0.6782908274614954\n",
      "learning rate 0.000999\n",
      "\n",
      "======epoch 2 loss====== 0.09120841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [02:13<34:41, 44.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 2 the macro-F1 on dev set is 0.6797603562239285\n",
      "learning rate 0.000998001\n",
      "best model updated; new best macro-F1 0.6797603562239285\n",
      "\n",
      "======epoch 3 loss====== 0.084764436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [02:56<33:51, 44.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 3 the macro-F1 on dev set is 0.6411568603863701\n",
      "learning rate 0.000997002999\n",
      "\n",
      "======epoch 4 loss====== 0.07891886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [03:41<33:12, 44.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 4 the macro-F1 on dev set is 0.6491294849007614\n",
      "learning rate 0.000996005996001\n",
      "\n",
      "======epoch 5 loss====== 0.07201616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [04:31<33:39, 45.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 5 the macro-F1 on dev set is 0.6596624534389918\n",
      "learning rate 0.000995009990004999\n",
      "\n",
      "======epoch 6 loss====== 0.07463326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [05:16<32:46, 45.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 6 the macro-F1 on dev set is 0.6742952156548185\n",
      "learning rate 0.000994014980014994\n",
      "\n",
      "======epoch 7 loss====== 0.06624379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [06:03<32:12, 46.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 7 the macro-F1 on dev set is 0.6618870920698706\n",
      "learning rate 0.0009930209650349789\n",
      "\n",
      "======epoch 8 loss====== 0.07310671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [06:47<31:10, 45.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 8 the macro-F1 on dev set is 0.6331740723728738\n",
      "learning rate 0.0009920279440699439\n",
      "\n",
      "======epoch 9 loss====== 0.07072724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [07:33<30:23, 45.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 9 the macro-F1 on dev set is 0.6586502597793511\n",
      "learning rate 0.0009910359161258739\n",
      "\n",
      "======epoch 10 loss====== 0.06233874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [08:18<29:27, 45.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 10 the macro-F1 on dev set is 0.6719150583074991\n",
      "learning rate 0.000990044880209748\n",
      "\n",
      "======epoch 11 loss====== 0.07321597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [09:04<28:52, 45.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 11 the macro-F1 on dev set is 0.6790704760724283\n",
      "learning rate 0.0009890548353295382\n",
      "\n",
      "======epoch 12 loss====== 0.06495786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [09:51<28:20, 45.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 12 the macro-F1 on dev set is 0.7078331415168136\n",
      "learning rate 0.0009880657804942088\n",
      "best model updated; new best macro-F1 0.7078331415168136\n",
      "\n",
      "======epoch 13 loss====== 0.06602712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [10:38<27:46, 46.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 13 the macro-F1 on dev set is 0.6610761899844078\n",
      "learning rate 0.0009870777147137145\n",
      "\n",
      "======epoch 14 loss====== 0.05831397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [11:21<26:32, 45.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 14 the macro-F1 on dev set is 0.6821604052111312\n",
      "learning rate 0.0009860906369990009\n",
      "\n",
      "======epoch 15 loss====== 0.050444797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [12:05<25:33, 45.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 15 the macro-F1 on dev set is 0.656842931219534\n",
      "learning rate 0.000985104546362002\n",
      "\n",
      "======epoch 16 loss====== 0.05490991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [12:50<24:45, 45.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 16 the macro-F1 on dev set is 0.6624148084430622\n",
      "learning rate 0.00098411944181564\n",
      "\n",
      "======epoch 17 loss====== 0.06446816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [13:34<23:46, 44.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 17 the macro-F1 on dev set is 0.647422453212752\n",
      "learning rate 0.0009831353223738242\n",
      "\n",
      "======epoch 18 loss====== 0.04928233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [14:19<23:04, 44.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 18 the macro-F1 on dev set is 0.6717951626319258\n",
      "learning rate 0.0009821521870514505\n",
      "\n",
      "======epoch 19 loss====== 0.0446603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [15:02<22:11, 44.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 19 the macro-F1 on dev set is 0.6782805723607472\n",
      "learning rate 0.000981170034864399\n",
      "\n",
      "======epoch 20 loss====== 0.044049878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [15:47<21:29, 44.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 20 the macro-F1 on dev set is 0.6601195869830296\n",
      "learning rate 0.0009801888648295347\n",
      "\n",
      "======epoch 21 loss====== 0.05107167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [16:30<20:28, 43.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 21 the macro-F1 on dev set is 0.681331112732898\n",
      "learning rate 0.000979208675964705\n",
      "\n",
      "======epoch 22 loss====== 0.04967882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [17:14<19:49, 44.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 22 the macro-F1 on dev set is 0.6805712008617039\n",
      "learning rate 0.0009782294672887404\n",
      "\n",
      "======epoch 23 loss====== 0.044649035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [17:57<18:55, 43.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 23 the macro-F1 on dev set is 0.6201383464863389\n",
      "learning rate 0.0009772512378214517\n",
      "\n",
      "======epoch 24 loss====== 0.05195353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [18:43<18:28, 44.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 24 the macro-F1 on dev set is 0.6794505524211143\n",
      "learning rate 0.0009762739865836303\n",
      "\n",
      "======epoch 25 loss====== 0.055412915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [19:31<18:14, 45.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 25 the macro-F1 on dev set is 0.6577581444254349\n",
      "learning rate 0.0009752977125970467\n",
      "\n",
      "======epoch 26 loss====== 0.05007513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [20:18<17:37, 45.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 26 the macro-F1 on dev set is 0.6625399421748094\n",
      "learning rate 0.0009743224148844496\n",
      "\n",
      "======epoch 27 loss====== 0.044385504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [21:08<17:18, 47.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 27 the macro-F1 on dev set is 0.6685689025059294\n",
      "learning rate 0.0009733480924695652\n",
      "\n",
      "======epoch 28 loss====== 0.04266066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [21:54<16:25, 46.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 28 the macro-F1 on dev set is 0.6340830831958405\n",
      "learning rate 0.0009723747443770956\n",
      "\n",
      "======epoch 29 loss====== 0.041861035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [22:40<15:31, 46.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 29 the macro-F1 on dev set is 0.6542411100274601\n",
      "learning rate 0.0009714023696327184\n",
      "\n",
      "======epoch 30 loss====== 0.044918973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [23:29<14:58, 47.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 30 the macro-F1 on dev set is 0.653039011198007\n",
      "learning rate 0.0009704309672630857\n",
      "\n",
      "======epoch 31 loss====== 0.044772033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [24:15<14:05, 46.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 31 the macro-F1 on dev set is 0.6498245165876251\n",
      "learning rate 0.0009694605362958226\n",
      "\n",
      "======epoch 32 loss====== 0.047125276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [25:02<13:17, 46.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 32 the macro-F1 on dev set is 0.6361280084633695\n",
      "learning rate 0.0009684910757595268\n",
      "\n",
      "======epoch 33 loss====== 0.03577512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [25:50<12:35, 47.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 33 the macro-F1 on dev set is 0.6451139999527634\n",
      "learning rate 0.0009675225846837673\n",
      "\n",
      "======epoch 34 loss====== 0.043637935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [26:35<11:36, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 34 the macro-F1 on dev set is 0.6817968754818455\n",
      "learning rate 0.0009665550620990835\n",
      "\n",
      "======epoch 35 loss====== 0.043045886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [27:21<10:48, 46.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 35 the macro-F1 on dev set is 0.6583771899801487\n",
      "learning rate 0.0009655885070369844\n",
      "\n",
      "======epoch 36 loss====== 0.036946934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-384d5f0d898f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             y_pred = make_batch_prediction(\n\u001b[1;32m---> 55\u001b[1;33m                 test_docs[idx:idx+batch_size], word_vectors, model, gpu)\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mpred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-05594b43e7e7>\u001b[0m in \u001b[0;36mmake_batch_prediction\u001b[1;34m(sent_list, word_vectors, model, use_gpu)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0minput_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_sents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mbatch_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-6d93d932f2b8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, all_text_vectors)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcnn_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_repr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mcv_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conv_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_text_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mcnn_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_repr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# print(cnn_repr.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-6d93d932f2b8>\u001b[0m in \u001b[0;36mget_conv_output\u001b[1;34m(self, input_matrix, conv, gpu)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# step 3: max-over-time pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mmaxp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_relu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mmaxp_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_relu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmaxp_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     74\u001b[0m         return F.max_pool1d(input, self.kernel_size, self.stride,\n\u001b[0;32m     75\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconds_Re\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool1d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     return torch.max_pool1d(\n\u001b[1;32m--> 457\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m max_pool1d = boolean_dispatch(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    for idx in range(0,len(train_docs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        sents = train_docs[idx:idx+batch_size]\n",
    "        if len(sents) == 0: break\n",
    "        y_target = torch.tensor([train_labels[idx:idx+batch_size]], dtype=torch.int64).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = make_batch_prediction(sents, word_vectors, model, gpu)\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 4+: clip the gradient, to avoid gradient explosion\n",
    "        nn.utils.clip_grad_value_(model.parameters(), clip_value=3.)\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        test_docs = dev_docs\n",
    "        test_labels = dev_labels\n",
    "        \n",
    "        for idx in range(0,len(test_docs),batch_size):\n",
    "            y_pred = make_batch_prediction(\n",
    "                test_docs[idx:idx+batch_size], word_vectors, model, gpu)\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "            predictions += pred_labels\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(test_labels, predictions,average='macro')\n",
    "        print('\\n---> after epoch {} the macro-F1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best macro-F1',f1)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.33712   , -0.25830999,  0.23726   ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.21691   ,  0.43643999, -0.46050999,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.0066365 , -0.1138    ,  0.07555   ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.40558001,  0.081697  ,  0.086256  ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.18073   , -0.0044191 ,  0.16498999,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.64249998, -0.14102   ,  0.60500002,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.33712   ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.21691   ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.0066365 ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.40558001,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.18073   ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.64249998,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.20437001, -0.1749    , -0.29712   , -0.25641   ,\n",
       "         -0.32635999,  0.23726   ],\n",
       "        [ 0.16430999,  0.22956   ,  0.094049  , -0.27074999,\n",
       "         -0.46357   , -0.46050999],\n",
       "        [ 0.041794  ,  0.24924   , -0.096662  , -0.01199   ,\n",
       "         -0.75915998,  0.07555   ],\n",
       "        ...,\n",
       "        [-0.34007001, -0.24131   ,  0.059717  , -0.34086999,\n",
       "          0.058154  ,  0.086256  ],\n",
       "        [-0.077146  , -0.40402001, -0.22853   , -0.3813    ,\n",
       "         -0.48012   ,  0.16498999],\n",
       "        [-0.084089  ,  0.054744  ,  0.29602   ,  0.13515   ,\n",
       "          0.053241  ,  0.60500002]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "word_vec_dim = 300 # make sure this number matches the embedding you use\n",
    "\n",
    "# Out-of-vocabulary (OOV) words: words that are not included in the pre-trained embedding model\n",
    "# There exist many ways to vectorize OOV words, e.g. use a random vector to represent all OOV words\n",
    "# Feel free to search and employ other ways to vectorize OOV words\n",
    "oov_vec = oov_vec = np.random.rand(word_vec_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    for i in range(largest_len-len(sent_words)):\n",
    "        vecs.append([0.]*word_vec_dim)\n",
    "    return np.array(np.transpose(vecs))\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors):\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "  \n",
    "# sanity check \n",
    "build_mini_batch(['hello world!','HELLO','this is a long sentence!'], word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_Clf(nn.Module):\n",
    "    def __init__(self, embd_dim, filter_size_list, filter_num_list, class_num, dp_rate=0.5, gpu=True):\n",
    "        super(CNN_Clf, self).__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        assert len(filter_size_list) == len(filter_num_list)\n",
    "        self.output_dim = class_num\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.fc = nn.Linear(np.sum(filter_num_list), class_num)\n",
    "        self.gpu = gpu\n",
    "        self.convs = self.build_convs(filter_size_list, filter_num_list, gpu)\n",
    "        if self.gpu:\n",
    "            self.to('cuda')\n",
    "            \n",
    "    def build_convs(self, f_sizes, f_nums, gpu):\n",
    "        convs = nn.ModuleList()\n",
    "        for fs, fn in zip(f_sizes, f_nums):\n",
    "            padding_size = fs-1\n",
    "            m = nn.Conv1d(self.embd_dim, fn, fs, padding=padding_size)\n",
    "            if gpu: m.to('cuda')\n",
    "            convs.append(m)\n",
    "        return convs\n",
    "        \n",
    "    def get_conv_output(self, input_matrix, conv, gpu):\n",
    "        # step 1: compute convolution \n",
    "        assert input_matrix.shape[1] == self.embd_dim\n",
    "        conv_output = conv(input_matrix)\n",
    "        # step 2: pass through an activation function \n",
    "        conv_relu = self.tanh(conv_output)\n",
    "        # step 3: max-over-time pooling\n",
    "        maxp = nn.MaxPool1d(conv_relu.shape[2])\n",
    "        maxp_output = maxp(conv_relu)\n",
    "        return maxp_output\n",
    "       \n",
    "    def forward(self, all_text_vectors):\n",
    "        cnn_repr = torch.tensor([])\n",
    "        if self.gpu: cnn_repr = cnn_repr.to('cuda')\n",
    "        for cv in self.convs:\n",
    "            cv_output = self.get_conv_output(all_text_vectors, cv, self.gpu)\n",
    "            cnn_repr = torch.cat((cnn_repr, cv_output), dim=1)\n",
    "        # print(cnn_repr.shape)\n",
    "        after_dp = self.dropout(cnn_repr.squeeze())\n",
    "        logit = self.fc(after_dp)\n",
    "        # the CrossEntropyLoss provided by pytorch includes softmax; so you do not need to include a softmax layer in your net\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5 # dropout rate\n",
    "filter_sizes = [2,3,4]\n",
    "filter_nums = [100]*len(filter_sizes)\n",
    "\n",
    "gpu = True # whether use gpu to accelerate the training\n",
    "model = CNN_Clf(word_vec_dim, filter_sizes, filter_nums, len(labels_list), dropout_rate, gpu)\n",
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 50 # number of epoch (i.e. number of iterations) # tried 10\n",
    "batch_size = 100 # earlier 50\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> macro-F1 on dev set is 0.22392575159822461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "    model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "    dev_predictions = []\n",
    "    for idx in range(0,len(dev_docs),batch_size):\n",
    "        x_data = build_mini_batch(dev_docs[idx:idx+batch_size], word_vectors)\n",
    "        if x_data.shape[0] == 0: continue # to avoid empty batch\n",
    "        # print(x_data.shape)\n",
    "        x_tensor = torch.tensor(x_data, dtype=torch.float)\n",
    "        if gpu:\n",
    "            x_tensor = x_tensor.to('cuda')\n",
    "        y_pred = model(x_tensor).cpu().detach().numpy()\n",
    "        # print(y_pred)\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred]\n",
    "        # print(pred_labels)\n",
    "        dev_predictions += pred_labels\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(dev_labels, dev_predictions,average='macro')\n",
    "    print('\\n---> macro-F1 on dev set is {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======epoch 0 loss====== 0.026407395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/50 [00:12<10:11, 12.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-F1 on dev set is 0.6891445274803374\n",
      "learning rate 0.0009175547491935324\n",
      "best model updated; new best macro-F1 0.6891445274803374\n",
      "\n",
      "======epoch 1 loss====== 0.023763936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 2/50 [00:24<09:47, 12.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 1 the macro-F1 on dev set is 0.660178282801715\n",
      "learning rate 0.0009166371944443389\n",
      "\n",
      "======epoch 2 loss====== 0.024963344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 3/50 [00:36<09:40, 12.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 2 the macro-F1 on dev set is 0.6793825270463361\n",
      "learning rate 0.0009157205572498945\n",
      "\n",
      "======epoch 3 loss====== 0.031653915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 4/50 [00:49<09:32, 12.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 3 the macro-F1 on dev set is 0.6711881640280652\n",
      "learning rate 0.0009148048366926446\n",
      "\n",
      "======epoch 4 loss====== 0.03379294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 5/50 [01:02<09:25, 12.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 4 the macro-F1 on dev set is 0.6906125192577546\n",
      "learning rate 0.000913890031855952\n",
      "best model updated; new best macro-F1 0.6906125192577546\n",
      "\n",
      "======epoch 5 loss====== 0.030656476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 6/50 [01:16<09:29, 12.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 5 the macro-F1 on dev set is 0.6739367621143553\n",
      "learning rate 0.000912976141824096\n",
      "\n",
      "======epoch 6 loss====== 0.02725152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 7/50 [01:29<09:25, 13.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 6 the macro-F1 on dev set is 0.6617372044016039\n",
      "learning rate 0.0009120631656822719\n",
      "\n",
      "======epoch 7 loss====== 0.029774535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▌        | 8/50 [01:43<09:17, 13.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 7 the macro-F1 on dev set is 0.6836387980979728\n",
      "learning rate 0.0009111511025165896\n",
      "\n",
      "======epoch 8 loss====== 0.026403824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 9/50 [01:58<09:25, 13.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 8 the macro-F1 on dev set is 0.6786795001199646\n",
      "learning rate 0.000910239951414073\n",
      "\n",
      "======epoch 9 loss====== 0.023886422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 10/50 [02:12<09:15, 13.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 9 the macro-F1 on dev set is 0.6766986985423395\n",
      "learning rate 0.0009093297114626589\n",
      "\n",
      "======epoch 10 loss====== 0.024918992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 11/50 [02:25<08:48, 13.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 10 the macro-F1 on dev set is 0.6647767747971888\n",
      "learning rate 0.0009084203817511963\n",
      "\n",
      "======epoch 11 loss====== 0.021200886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 12/50 [02:38<08:36, 13.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 11 the macro-F1 on dev set is 0.6798763462305544\n",
      "learning rate 0.000907511961369445\n",
      "\n",
      "======epoch 12 loss====== 0.023119474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 13/50 [02:53<08:38, 14.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 12 the macro-F1 on dev set is 0.6700476344618269\n",
      "learning rate 0.0009066044494080756\n",
      "\n",
      "======epoch 13 loss====== 0.025275856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 14/50 [03:07<08:15, 13.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 13 the macro-F1 on dev set is 0.6747162810999563\n",
      "learning rate 0.0009056978449586675\n",
      "\n",
      "======epoch 14 loss====== 0.027991716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 15/50 [03:19<07:48, 13.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 14 the macro-F1 on dev set is 0.6707611524067221\n",
      "learning rate 0.0009047921471137089\n",
      "\n",
      "======epoch 15 loss====== 0.02909395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 16/50 [03:31<07:21, 12.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 15 the macro-F1 on dev set is 0.6662470545194195\n",
      "learning rate 0.0009038873549665952\n",
      "\n",
      "======epoch 16 loss====== 0.030927787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▍      | 17/50 [03:44<07:07, 12.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 16 the macro-F1 on dev set is 0.6743320170792\n",
      "learning rate 0.0009029834676116286\n",
      "\n",
      "======epoch 17 loss====== 0.027471423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 18/50 [03:57<06:53, 12.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 17 the macro-F1 on dev set is 0.6721977101427257\n",
      "learning rate 0.000902080484144017\n",
      "\n",
      "======epoch 18 loss====== 0.025780404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 19/50 [04:10<06:38, 12.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 18 the macro-F1 on dev set is 0.6730507298266677\n",
      "learning rate 0.000901178403659873\n",
      "\n",
      "======epoch 19 loss====== 0.031068284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 20/50 [04:22<06:20, 12.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 19 the macro-F1 on dev set is 0.6633014827371916\n",
      "learning rate 0.0009002772252562131\n",
      "\n",
      "======epoch 20 loss====== 0.02394405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 21/50 [04:34<06:05, 12.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 20 the macro-F1 on dev set is 0.6558879150298125\n",
      "learning rate 0.0008993769480309569\n",
      "\n",
      "======epoch 21 loss====== 0.024244646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████▍     | 22/50 [04:47<05:53, 12.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 21 the macro-F1 on dev set is 0.6767019925010072\n",
      "learning rate 0.0008984775710829259\n",
      "\n",
      "======epoch 22 loss====== 0.029775122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 23/50 [05:00<05:40, 12.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 22 the macro-F1 on dev set is 0.6810669644988718\n",
      "learning rate 0.000897579093511843\n",
      "\n",
      "======epoch 23 loss====== 0.025765184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 24/50 [05:12<05:28, 12.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 23 the macro-F1 on dev set is 0.694313418732037\n",
      "learning rate 0.0008966815144183311\n",
      "best model updated; new best macro-F1 0.694313418732037\n",
      "\n",
      "======epoch 24 loss====== 0.033281982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 25/50 [05:24<05:13, 12.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 24 the macro-F1 on dev set is 0.6892315241947349\n",
      "learning rate 0.0008957848329039128\n",
      "\n",
      "======epoch 25 loss====== 0.026209507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 26/50 [05:37<04:58, 12.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 25 the macro-F1 on dev set is 0.6933689932846885\n",
      "learning rate 0.0008948890480710088\n",
      "\n",
      "======epoch 26 loss====== 0.026742922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 27/50 [05:49<04:43, 12.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 26 the macro-F1 on dev set is 0.6680765706722999\n",
      "learning rate 0.0008939941590229378\n",
      "\n",
      "======epoch 27 loss====== 0.026785314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████▌    | 28/50 [06:01<04:29, 12.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 27 the macro-F1 on dev set is 0.6786795001199646\n",
      "learning rate 0.0008931001648639148\n",
      "\n",
      "======epoch 28 loss====== 0.023602994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 29/50 [06:14<04:21, 12.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 28 the macro-F1 on dev set is 0.6606237824561685\n",
      "learning rate 0.0008922070646990509\n",
      "\n",
      "======epoch 29 loss====== 0.028463474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 30/50 [06:26<04:08, 12.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 29 the macro-F1 on dev set is 0.6601199713181862\n",
      "learning rate 0.0008913148576343518\n",
      "\n",
      "======epoch 30 loss====== 0.026599128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 31/50 [06:39<03:56, 12.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 30 the macro-F1 on dev set is 0.6703689073943366\n",
      "learning rate 0.0008904235427767174\n",
      "\n",
      "======epoch 31 loss====== 0.027365576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████▍   | 32/50 [06:52<03:47, 12.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 31 the macro-F1 on dev set is 0.6797784463929883\n",
      "learning rate 0.0008895331192339407\n",
      "\n",
      "======epoch 32 loss====== 0.025940955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|██████▌   | 33/50 [07:06<03:41, 13.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 32 the macro-F1 on dev set is 0.6818009209904052\n",
      "learning rate 0.0008886435861147067\n",
      "\n",
      "======epoch 33 loss====== 0.025867952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 34/50 [07:19<03:27, 12.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 33 the macro-F1 on dev set is 0.6650113089832845\n",
      "learning rate 0.000887754942528592\n",
      "\n",
      "======epoch 34 loss====== 0.02576759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 35/50 [07:31<03:13, 12.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 34 the macro-F1 on dev set is 0.6699879720890454\n",
      "learning rate 0.0008868671875860634\n",
      "\n",
      "======epoch 35 loss====== 0.02708745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 36/50 [07:46<03:08, 13.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 35 the macro-F1 on dev set is 0.6800930435488454\n",
      "learning rate 0.0008859803203984774\n",
      "\n",
      "======epoch 36 loss====== 0.023854643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▍  | 37/50 [08:00<02:58, 13.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 36 the macro-F1 on dev set is 0.6743269160990681\n",
      "learning rate 0.0008850943400780789\n",
      "\n",
      "======epoch 37 loss====== 0.025516327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 38/50 [08:14<02:42, 13.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 37 the macro-F1 on dev set is 0.6869531159660857\n",
      "learning rate 0.0008842092457380008\n",
      "\n",
      "======epoch 38 loss====== 0.022473784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 39/50 [08:26<02:25, 13.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 38 the macro-F1 on dev set is 0.6676027054004464\n",
      "learning rate 0.0008833250364922628\n",
      "\n",
      "======epoch 39 loss====== 0.020327482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 40/50 [08:39<02:11, 13.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 39 the macro-F1 on dev set is 0.6886769608170553\n",
      "learning rate 0.0008824417114557706\n",
      "\n",
      "======epoch 40 loss====== 0.023014724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 41/50 [08:52<01:56, 13.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 40 the macro-F1 on dev set is 0.6879662460251472\n",
      "learning rate 0.0008815592697443149\n",
      "\n",
      "======epoch 41 loss====== 0.023542993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▍ | 42/50 [09:04<01:43, 12.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 41 the macro-F1 on dev set is 0.6793646572849161\n",
      "learning rate 0.0008806777104745705\n",
      "\n",
      "======epoch 42 loss====== 0.022462783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 43/50 [09:17<01:29, 12.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 42 the macro-F1 on dev set is 0.6931743061366736\n",
      "learning rate 0.000879797032764096\n",
      "\n",
      "======epoch 43 loss====== 0.026712397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 44/50 [09:31<01:18, 13.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 43 the macro-F1 on dev set is 0.6929331514056518\n",
      "learning rate 0.0008789172357313319\n",
      "\n",
      "======epoch 44 loss====== 0.023225896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 45/50 [09:44<01:05, 13.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 44 the macro-F1 on dev set is 0.6766799363383911\n",
      "learning rate 0.0008780383184956006\n",
      "\n",
      "======epoch 45 loss====== 0.027342288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 46/50 [09:59<00:55, 13.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 45 the macro-F1 on dev set is 0.6815400595812876\n",
      "learning rate 0.000877160280177105\n",
      "\n",
      "======epoch 46 loss====== 0.023316776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▍| 47/50 [10:14<00:42, 14.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 46 the macro-F1 on dev set is 0.671113023522662\n",
      "learning rate 0.0008762831198969279\n",
      "\n",
      "======epoch 47 loss====== 0.018662807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 48/50 [10:26<00:27, 13.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 47 the macro-F1 on dev set is 0.6770977464785093\n",
      "learning rate 0.000875406836777031\n",
      "\n",
      "======epoch 48 loss====== 0.021740027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 49/50 [10:39<00:13, 13.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 48 the macro-F1 on dev set is 0.6834390312032406\n",
      "learning rate 0.000874531429940254\n",
      "\n",
      "======epoch 49 loss====== 0.026456615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 50/50 [10:52<00:00, 13.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 49 the macro-F1 on dev set is 0.6877554935184891\n",
      "learning rate 0.0008736568985103137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    for idx in range(0,len(train_docs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        x_data = build_mini_batch(train_docs[idx:idx+batch_size], word_vectors)\n",
    "        if x_data.shape[0] == 0: continue # to avoid empty batch\n",
    "        y_target = torch.tensor([train_labels[idx:idx+batch_size]], dtype=torch.int64).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        x_tensor = torch.tensor(x_data, dtype=torch.float)\n",
    "        if gpu:\n",
    "            x_tensor = x_tensor.to('cuda')\n",
    "        y_pred = model(x_tensor)\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        dev_predictions = []\n",
    "        for idx in range(0,len(dev_docs),batch_size):\n",
    "            x_data = build_mini_batch(dev_docs[idx:idx+batch_size], word_vectors)\n",
    "            if x_data.shape[0] == 0: continue # to avoid empty batch\n",
    "            x_tensor = torch.tensor(x_data, dtype=torch.float)\n",
    "            if gpu:\n",
    "                x_tensor = x_tensor.to('cuda')\n",
    "            y_pred = model(x_tensor).cpu().detach().numpy()\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred]\n",
    "            dev_predictions += pred_labels\n",
    "            # print(pred_labels)\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(dev_labels, dev_predictions,average='macro')\n",
    "        print('\\n---> after epoch {} the macro-F1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best macro-F1',f1)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE YOUR TRAINED MODEL\n",
    "After you have obtained the best model, save your trained model and other necessary components to a file. The markers will load your model from the saved file and test your trained model on some held-out test data. Make sure that you have included all necessary files to re-run your model. **The markers will NOT re-run your code to train your model; instead, they will directly use your trained model to run the test**. \n",
    "\n",
    "Below is the sample code for saving the model and other necessary components, using the *pickle* package in Python. *You should adjust the code to save all necessary components for re-running your model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-7e810a48bae3>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-7e810a48bae3>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    'class_num' : 2\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save model and other necessary components of your model\n",
    "# DO NOT include the embedding files in your submission\n",
    "\n",
    "all_info_want_to_save = {\n",
    "    'input_dim': word_vec_dim,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'neural_weights': best_model,\n",
    "    'oov_vector': oov_vec\n",
    "    'class_num' : class_num\n",
    "}\n",
    "save_path = open(\"cw2_sample_saved_file.pickle\",\"wb\")\n",
    "pickle.dump(all_info_want_to_save, save_path)\n",
    "save_path.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT :\n",
    "\n",
    "#### Please note that some analysis/experiments were performed in tensorflow and the tf code used is commented below. My final model and results are saved in pytorch.To provide the answers below I have used many resources online from pytorch website,tensorflow website, stackoverflow, and excerpts from medium articles on the relevant subject matter.\n",
    "\n",
    "#### 1.How you use the data to develop your model, e.g. how to split the data into train/dev/test sets, and how you clean/normalize the data;\n",
    "\n",
    "##### This data is class imbalanced. To split this data we have few options :\n",
    "##### 1. Under sampling : Reducing the number of samples in consideration from the class with greater number of samples. An aspect to be careful about while undersampling is not losing critical information.\n",
    "######   This can be randomized and aggregated as well(similar to ensemble models) for consistency. From my observation in tensoflow and an attempt at using the imblearn package, undersampling does marginally better many times.\n",
    "\n",
    "###### 2. Over sampling : Oversampling can involve using multiple techniques, one way to do it is sampling the minority class more than one time or using synthetic oversampling. Synthetic oversampling is create an artificial data point by taking the vector between one of those k neighbors, and the current data point. Multiply this vector by a random number x which lies between 0 and 1.This vector results in the new data point being created.\n",
    "\n",
    "###### I have settled for the 60/20/20 split of the dataset in the interest of time.Also, I noticed that oversampling does not increase performance greatly and undersampling could result in loss of important data. There is a lot of conditions to permutate and I decided to try to make the model better with existing conditions for this assignment.\n",
    "\n",
    "###### With respect to cleaning and normalization, I have tried both stemming,lemmatization with stop words, punctuation, numbers(if any ) removal for the machine learning algorithm.\n",
    "###### For the neural network I have not cleaned the data much since the aim of the architecture is sequence modelling.\n",
    "\n",
    "\n",
    "#### 2.Which techniques or embeddings you have used to represent the texts and why you choose the use them ?\n",
    "\n",
    "##### I have used the Glove embedding. Glove embeddings are word based models, they take in words as inputs and provide word embedded vectors as ouput.Glove embedding is very good at capturing the semantics of analogy, it can capture a more general flavour of the context.\n",
    "##### When compared with word2vec, word2vec does not have any explicit global information embedded in it by default. GloVe creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words.I have retained the default embedding(Glove) for the very same reason.\n",
    "\n",
    "##### Each row of the matrix represents a word, while each column represents the contexts that words can appear in. The matrix values represent the frequency a word appears in a given context. Then, dimensionality reduction/mapping is applied to this matrix to create the resulting embedding matrix (each row will be a word’s embedding vector).\n",
    "\n",
    "\n",
    "\n",
    "#### 3.Which neural architecture(s) you have used to develop the classifier and why you choose to use them? 4.Which techniques (e.g. optimizers, regularization mechanisms, hyper-parameter tuning tricks) you have used to train the neural model and why you choose to use them.\n",
    "\n",
    "##### Answering q3 and q4 :\n",
    "\n",
    "##### I have tried 3 neural architectures :\n",
    "##### 1. Simple layered architecture with two linear layers with dropout and relu activation layer.\n",
    "##### 2. LSTM\n",
    "##### 3. CNN \n",
    "\n",
    "##### In the first neural architecture used : \n",
    "        #self.hidden_layer = nn.Linear(input_dim, input_dim*2)\n",
    "        #self.output_layer = nn.Linear(input_dim*2, out_dim)\n",
    "        #self.dropout = nn.Dropout(dp_rate)\n",
    "        #self.relu = torch.nn.ReLU()\n",
    "\n",
    "##### nn :A fully-connected ReLU network with one hidden layer, trained to predict y from x by minimizing squared Euclidean distance.This implementation uses the nn package from PyTorch to build the network. PyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks; this is where the nn package can help. The nn package defines a set of Modules, which you can think of as a neural network layer that has produces output from input and may have some trainable weights.\n",
    "\n",
    "##### Linear layer: A linear operation in which every input is connected to every output by a weight (so there are n_inputs * n_outputs weights. Generally followed by a non-linear activation function. ie: Applies a linear transformation to the incoming data, i.e. //y= Ax+b//. The input tensor given in forward(input) must be either a vector (1D tensor) or matrix (2D tensor).\n",
    "\n",
    "##### Hidden layer role: If the input is a matrix, then each row is assumed to be an input sample of given batch.The module automatically creates the weight and bias tensors which we'll use in the forward method. You can access the weight and bias tensors once the network once it's create at net.hidden.weight and net.hidden.bias.\n",
    "\n",
    "##### Dropout layer: In passing 0.5, every hidden unit (neuron) is set to 0 with a probability of 0.5.There’s a 50% change that the output of a given neuron will be forced to 0. Dropout can help a model generalize by randomly setting the output for a given neuron to 0. In setting the output to 0, the cost function becomes more sensitive to neighbouring neurons changing the way the weights will be updated during the process of backpropagation.\n",
    "\n",
    "##### Relu layer: The ReLU is a function with the form of y=max(0,x). The layer applies the function to all  \n",
    "\n",
    "\n",
    "##### In the second neural architecture used : LSTM\n",
    "\n",
    "##### embd_dim = 300\n",
    "##### hidden_dim = 300\n",
    "##### rnn_type = 'bilstm'\n",
    "##### pooler_type = 'avg'\n",
    "##### dropout = 0.5\n",
    "\n",
    "##### embed_dim – total dimension of the model.\n",
    "##### num_heads – parallel attention heads.\n",
    "##### dropout – a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "##### bias – add bias as module parameter. Default: True.\n",
    "\n",
    "##### Optimizer used -  Adam optimizer :                                                                                                Adaptive Moment Estimation (Adam) [14] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.\n",
    "\n",
    "#### Very little hyperparameter tuning tried while working with this algorithm.\n",
    "\n",
    "##### In the second neural architecture used : CNN \n",
    "\n",
    "n_epochs = 50 # number of epoch (i.e. number of iterations) # tried 10\n",
    "batch_size = 100 # earlier 50\n",
    "lr = 0.001 # initial learning rate\n",
    "Adam optimizer used\n",
    "\n",
    "Hyperparameters played with is number of epochs, batch size and learning rate. After playing around with these parameters, I have settled on using 50 epochs. Initially when I tried using 20 epochs with 50 sample batch size, There was very slow increase in f1 score. I have also looked at the risk of overfitting by increasing the number of epochs and batch_size. After observing the learning rate and judging the nature of the dataset I felt more number of epochs might learn the data a lot more. I also looked at the stability of the algorithm with multiple iterations. \n",
    "\n",
    "\n",
    "#### I have finally used the model that has the best F1 score, displayed stability in multiple trials does not show indicators of overfitting or underfitting. I have picked CNN as the algorithm of choice\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5.How you compare and analyze the performance of your developed models\n",
    "\n",
    "As mentioned above. The performance criteria were as follows :\n",
    "\n",
    "1. Look at macro F1 score since the classes are imbalanced.\n",
    "2. Since the weights are initialized randomly - repeat for consistency. \n",
    "3. Observe effect of fine tuning hyper parameters\n",
    "4. Observe effect of data cleaning\n",
    "\n",
    "Clearly the neural network performs better than the machine learning model for this type of data where the data is more imbalanced and the differences between the classes are harder to detect and requires sequences or convolutional networks to identify multiple patterns\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_ratio, dev_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "# train_docs = docs[:int(len(docs)*train_ratio)]\n",
    "# train_labels = labels[:int(len(docs)*train_ratio)]\n",
    "# train_labels_text = raw_labels[:int(len(docs)*train_ratio)]\n",
    "\n",
    "# dev_docs = docs[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "# dev_labels = labels[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "\n",
    "# test_docs = docs[-int(len(docs)*(test_ratio)):]\n",
    "# test_labels = labels[-int(len(docs)*(test_ratio)):]\n",
    "\n",
    "# print(vocab_size = 40000\n",
    "# oov_tok = '<OOV>'\n",
    "# tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "# tokenizer.fit_on_texts(train_docs)\n",
    "# word_index = tokenizer.word_index\n",
    "# dict(list(word_index.items())[0:10])'train size {}, dev size {}, test size {}'.format(len(train_labels), len(dev_labels), len(test_labels)))\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_docs)\n",
    "# train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "# train_padded_new = train_padded.reshape(1, train_padded.shape[0], train_padded.shape[1])\n",
    "# validation_sequences = tokenizer.texts_to_sequences(dev_docs)\n",
    "# validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "# val_padded_new = validation_padded.reshape(1, validation_padded.shape[0], validation_padded.shape[1])\n",
    "# label_tokenizer = Tokenizer()\n",
    "# label_tokenizer.fit_on_texts(train_labels_text)\n",
    "\n",
    "# training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels_text))\n",
    "# validation_label_seq = np.array(label_tokenizer.texts_to_sequences(dev_labels_text))\n",
    "# embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "# hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "#                            dtype=tf.string, trainable=True)\n",
    "# hub_layer(train_docs)\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(hub_layer)\n",
    "# # model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# model.summary()\n",
    "# from tensorflow.keras.backend import backend as K\n",
    "\n",
    "# def recall_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     recall = true_positives / (possible_positives + K.epsilon())\n",
    "#     return recall\n",
    "\n",
    "# def precision_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#     return precision\n",
    "\n",
    "# def f1_score(y_true, y_pred):\n",
    "#     precision = precision_m(y_true, y_pred)\n",
    "#     recall = recall_m(y_true, y_pred)\n",
    "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# import numpy as np\n",
    "\n",
    "# def create_f1():\n",
    "#     def f1_function(y_true, y_pred):\n",
    "#         y_pred_binary = tf.where(y_pred>=0.5, 1., 0.)\n",
    "#         tp = tf.reduce_sum(y_true * y_pred_binary)\n",
    "#         predicted_positives = tf.reduce_sum(y_pred_binary)\n",
    "#         possible_positives = tf.reduce_sum(y_true)\n",
    "#         return tp, predicted_positives, possible_positives\n",
    "#     return f1_function\n",
    "\n",
    "\n",
    "# class F1_score(keras.metrics.Metric):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "#         self.f1_function = create_f1()\n",
    "#         self.tp_count = self.add_weight(\"tp_count\", initializer=\"zeros\")\n",
    "#         self.all_predicted_positives = self.add_weight('all_predicted_positives', initializer='zeros')\n",
    "#         self.all_possible_positives = self.add_weight('all_possible_positives', initializer='zeros')\n",
    "\n",
    "#     def update_state(self, y_true, y_pred,sample_weight=None):\n",
    "#         tp, predicted_positives, possible_positives = self.f1_function(y_true, y_pred)\n",
    "#         self.tp_count.assign_add(tp)\n",
    "#         self.all_predicted_positives.assign_add(predicted_positives)\n",
    "#         self.all_possible_positives.assign_add(possible_positives)\n",
    "\n",
    "#     def result(self):\n",
    "#         precision = self.tp_count / self.all_predicted_positives\n",
    "#         recall = self.tp_count / self.all_possible_positives\n",
    "#         f1 = 2*(precision*recall)/(precision+recall)\n",
    "#         return f1\n",
    "\n",
    "# X = np.random.random(size=(1000, 10))     \n",
    "# Y = np.random.randint(0, 2, size=(1000,))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# # model = tf.keras.Sequential()\n",
    "# # model.add(hub_layer)\n",
    "# # model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "# # model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=[F1_score()])\n",
    "# history = model.fit(train_padded.reshape(-1),\n",
    "#                     epochs=20,\n",
    "# #                     validation_data=validation_padded.reshape(-1),\n",
    "#                     verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

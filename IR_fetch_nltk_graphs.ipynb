{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-requisites:\n",
    "    1. Define seperate lists of independent search words like 'killed' and 'injured ' will have two different lists.\n",
    "    2. expand list by using nltk synonyms functionality and try to ensure thorough list.\n",
    "       To add : checks on grammar and rootword --Improvement required\n",
    "    3. define function that will clean the data to remove any hyphen between words and other cleaning if required\n",
    "Algorithm:\n",
    "    1. fetch sentence with word present(preferable)\n",
    "    2. set dependency tree and convert to networkx\n",
    "    3. shortest path with condition thatit should not traverse node of other list\n",
    "    4. handle case with multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Rishab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import matplotlib as plt\n",
    "from itertools import chain\n",
    "from nltk import Tree\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(dict.fromkeys(words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonym =[]\n",
    "    for syn_word in wn.synsets(word):\n",
    "        for sw in syn_word.lemmas():\n",
    "            synonym.append(sw.name())\n",
    "    return list(synonym)\n",
    "\n",
    "# def get_verb_tenses(word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listfetch(wordyouaresearchingfor):\n",
    "    return list(set(get_synonyms(wordyouaresearchingfor) +[wordyouaresearchingfor]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary searches\n",
    "\n",
    "\n",
    "killed_set = list(set(get_synonyms(\"killed\") +[\"killed\"] + [\"claimed\"]))\n",
    "died = list(set(get_synonyms(\"died\") + [\"died\"]))\n",
    "injured_set = list(set(get_synonyms(\"injured\") +[\"injured\"]))\n",
    "homeless_set = list(set(get_synonyms(\"homeless\") +[\"homeless\"]))\n",
    "destroyed_set = list(set(get_synonyms(\"destroyed\") +[\"destroyed\"]))\n",
    "displaced_set = list(set(get_synonyms(\"displaced\") +[\"displaced\"] +[\"drowned\"] + ['displacement']))\n",
    "pressure_set = list(set(get_synonyms(\"pressure\") +[\"pressure\"]))\n",
    "speed_set = list(set(get_synonyms(\"speed\") +[\"speed\"] + ['Category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def merge_lists(list1,*args):\n",
    "    return list1 + list(chain(*args))\n",
    "\n",
    "other_list= merge_lists(injured_set,homeless_set,destroyed_set,displaced_set,pressure_set,speed_set)\n",
    "died_set = merge_lists(killed_set,died)\n",
    "other_list_lower = list(map(str.lower,other_list))\n",
    "died_set_lower = list(map(str.lower,died_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for small_list in other_list:\n",
    "    temp += small_list\n",
    "newlist = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add condition if required\n",
    "#noticed in encyclopedia\n",
    "def dict_check(word):\n",
    "    if '-' in word:\n",
    "        word_raw = word.replace('-', '')\n",
    "        word_rep = wordnet_lemmatizer.lemmatize(word_raw)\n",
    "        if word_rep in dictionary:\n",
    "            return word_raw\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hurricane'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_check(\"hur-ricane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_graph(sentence):\n",
    "    edges = []\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token.lower_),\n",
    "                          '{0}'.format(child.lower_)))\n",
    "    graph = nx.Graph(edges)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_label(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    cardinal_list={}\n",
    "    for i,ent in enumerate(doc.ents):\n",
    "        cardinal_list.update({ent.text:ent.label_})\n",
    "    return cardinal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_entity_fetch(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entity_dict={}\n",
    "    for i,ent in enumerate(doc.ents):\n",
    "        entity_dict.update({ent.label_:ent.text})\n",
    "        \n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinal_check(dict1,dict2):\n",
    "    card_dict_dict = {}\n",
    "    cardinal_word_dict = []\n",
    "    final_dict ={}\n",
    "\n",
    "    for (key1,value1) in dict1.items():\n",
    "        print(key1,value1)\n",
    "        for (key2,value3) in dict2.items():\n",
    "            print(key2,value3)\n",
    "            for (word,value2) in value1.items():\n",
    "                print(word,value2)\n",
    "            \n",
    "                if word == key2 and value3 == 'CARDINAL':\n",
    "                    cardinal_word_dict.append({word:value2})\n",
    "                     \n",
    "                else:\n",
    "                    pass\n",
    "        resp_list = cardinal_word_dict\n",
    "        card_dict_dict.update({key1:resp_list})\n",
    "        print(card_dict_dict)\n",
    "        cardinal_word_dict.clear()\n",
    "        print(card_dict_dict)\n",
    "    return card_dict_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \" three people died and 500 were injured due to a hurricane of Category 2 \"\n",
    "sen = sentence.lower()\n",
    "c = fetch_label(sen)\n",
    "otherwordlist_dict = shortestpath(graph,sentence,other_list_lower)\n",
    "cardinal_check(otherwordlist_dict,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cardinal(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    cardinal_list=[]\n",
    "    for i,ent in enumerate(doc.ents):\n",
    "        if ent.label_ == 'CARDINAL':\n",
    "            cardinal_list.append(ent.text)\n",
    "    return cardinal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100,000', '134']"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_cardinal(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence\n",
    "cardinal_list=[]\n",
    "doc = nlp(sentence)\n",
    "for i,ent in enumerate(doc.ents):\n",
    "    if ent.label_ == 'CARDINAL':\n",
    "        cardinal_list.append(ent.text)\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100,000', '134']"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cardinal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_availability_check(sentence, searchlist): \n",
    "    available_list =[]\n",
    "    s = sentence.split(\" \")\n",
    "    sentence = list(map(str.lower,s))\n",
    "    for word in sentence:\n",
    "        for searchword in searchlist:\n",
    "            if (searchword == word):\n",
    "                available_list.append(searchword)\n",
    "#             elif(searchword == # can try lemma and tokenization after this if required\n",
    "            elif(searchword == dict_check(word)):\n",
    "                available_list.append(searchword)\n",
    "            elif(searchword == wordnet_lemmatizer.lemmatize(word)):\n",
    "                available_list.append(word)\n",
    "            elif(wordnet_lemmatizer.lemmatize(searchword) == word):\n",
    "                available_list.append(word)\n",
    "            else:\n",
    "                pass\n",
    "    return list(set(available_list))\n",
    "#returns list of available words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestpath(graph,sentence,searchlist):\n",
    "    spath_val = {}\n",
    "    availablelist = word_availability_check(sentence,searchlist)\n",
    "    for searchword in availablelist:\n",
    "        next_word_dict = nx.single_source_shortest_path_length(graph,searchword)\n",
    "        spath_val.update({searchword:next_word_dict})\n",
    "    return spath_val\n",
    "#returns dictionary with key and shortest path value from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestpath_text(graph,sentence,searchlist):\n",
    "    spath_val = []\n",
    "    \n",
    "    availablelist = word_availability_check(sentence,searchlist)\n",
    "    cardinal_list = fetch_cardinal(sentence)\n",
    "    for searchword in availablelist:\n",
    "#         print(searchword)\n",
    "        for cardinal_word in cardinal_list:\n",
    "#             print(cardinal_word)\n",
    "            next_word_dict = nx.shortest_path(graph,searchword,cardinal_word)\n",
    "#             print(next_word_dict)\n",
    "            spath_val.append(next_word_dict)\n",
    "            \n",
    "    return spath_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_shortest_path(spath_val_output):\n",
    "    length_get = []\n",
    "    \n",
    "    for l in spath_val_output:\n",
    "        \n",
    "        length_get.append((l[0],l[-1],len(l)))\n",
    "    \n",
    "    return length_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "\n",
    "def accumulate(l):\n",
    "    it = itertools.groupby(l, operator.itemgetter(0))\n",
    "    for key,subiter in it:\n",
    "       yield key, min(item[1] for item in subiter)\n",
    "\n",
    "# list(accumulate(a))\n",
    "minValue = [i for i in a if i[1] == min(x[1] for x in a)]\n",
    "minValue\n",
    "Output = ([min(c) for a, b in itertools.groupby(Input, lambda y: y[0])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_fetch(sentence,searchlist,otherwordlist):\n",
    "    fetch_cardinal = {}\n",
    "    sentence = sentence.lower()\n",
    "    doc = nlp(sentence)\n",
    "    tree = [to_nltk_tree(sent.root) for sent in doc.sents]\n",
    "    print([to_nltk_tree(sent.root).pretty_print() for sent in doc.sents])\n",
    "    graph = fetch_graph(sentence)\n",
    "    context_dict = fetch_label(sentence)\n",
    "    spath_val_output_diedset = shortestpath_text(graph,sentence,died_set_lower)\n",
    "    l1 = length_shortest_path(spath_val_output_diedset)\n",
    "    spath_val_output_otherset = shortestpath_text(graph,sentence,other_list_lower)\n",
    "    l2 = length_shortest_path(spath_val_output_otherset)\n",
    "    final_list = l1+l2\n",
    "    df = pd.DataFrame(final_list)\n",
    "    info = df.loc[df.groupby(1)[2].idxmin()]\n",
    "\n",
    "    return info\n",
    "\n",
    "#     search_path_dict = shortestpath(graph,sentence,searchlist)\n",
    "#     otherwordlist_dict = shortestpath(graph,sentence,otherwordlist)    \n",
    "#     search_path_dict_filtered = cardinal_check(search_path_dict,context_dict)\n",
    "#     otherwordlist_dict_filtered = cardinal_check(otherwordlist_dict,context_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              forced                                                                                                    \n",
      "  ______________________________________________________________________________________________|__________                                                                                              \n",
      " |    |    |   |                      |                                       |                          caused                                                                                         \n",
      " |    |    |   |                      |                                       |                  __________|______________________________                                                               \n",
      " |    |    |   |                      |                                       |                 |     |            |                   claimed                                                          \n",
      " |    |    |   |                      |                                       |                 |     |            |                      |                                                              \n",
      " |    |    |   |                      |                                       |                 |     |            |                    lives                                                           \n",
      " |    |    |   |                      |                                       |                 |     |            |                 _____|________                                                      \n",
      " |    |    |   |                      |                                       |                 |     |            |                |          inflorida                                                \n",
      " |    |    |   |                      |                                       |                 |     |            |                |      ________|________                                             \n",
      " |    |    |   |                    during                                    |                 |     |            |                |     |              georgia                                        \n",
      " |    |    |   |                      |                                       |                 |     |            |                |     |         ________|_____________                               \n",
      " |    |    |   |                   sojourn                               displacement           |     |         billion             |     |        |                 pennsylvania                       \n",
      " |    |    |   |    __________________|__________________________      _______|__________       |     |     _______|__________      |     |        |         _____________|________                      \n",
      " |    |    |   |   |   |        |                 |              in   |                  of     |     |    |       |     |    in    |     |        |        |                     york                  \n",
      " |    |    |   |   |   |        |                 |              |    |                  |      |     |    |       |     |    |     |     |        |        |      ________________|______               \n",
      " |    |    |   |   |   |       day             seaboard         june  |                people   |     |    |       |     |  damage  |     |        |        |     |       |            virginia         \n",
      " |    |    |   |   |   |    ____|_____       _____|________      |    |                  |      |     |    |       |     |    |     |     |        |        |     |       |         ______|________      \n",
      " ,  agnes  ,   .   a   up five        -    the          eastern 1972 the              100,000   ,    and nearly    $     2  flood  134    ,        ,        ,    new      ,        ,     and    maryland\n",
      "\n",
      "[None]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>displacement</td>\n",
       "      <td>100,000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claimed</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  2\n",
       "2  displacement  100,000  4\n",
       "1       claimed      134  3"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"During a five-day sojourn up the eastern \\\n",
    "seaboard in June 1972, Agnes forced the displacement \\\n",
    "of 100,000 people, caused nearly \\\n",
    "$2 billion in flood damage, and claimed 134 lives in\\\n",
    "Florida, Georgia, Pennsylvania, New York, Virginia,\\\n",
    "and Maryland.\"\n",
    "df1 = info_fetch(sentence,died_set_lower,other_list_lower)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>ORG</th>\n",
       "      <th>CARDINAL</th>\n",
       "      <th>MONEY</th>\n",
       "      <th>GPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>June 1972</td>\n",
       "      <td>Agnes</td>\n",
       "      <td>134</td>\n",
       "      <td>nearly $2 billion</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE    ORG CARDINAL              MONEY       GPE\n",
       "0  June 1972  Agnes      134  nearly $2 billion  Maryland"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_entity = other_entity_fetch(sentence)\n",
    "df2 = pd.DataFrame.from_dict(other_entity,orient='index')\n",
    "df2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         killed                   \n",
      "  _________|___________            \n",
      " |    |    |        injured       \n",
      " |    |    |      _____|______     \n",
      " |    |  people  |     |      in  \n",
      " |    |    |     |     |      |    \n",
      "were and  two    15   were  haikou\n",
      "\n",
      "[None]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>injured</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>killed</td>\n",
       "      <td>two</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1  2\n",
       "3  injured   15  2\n",
       "0   killed  two  3"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Two people were killed and 15 were injured in Haikou\"\n",
    "df1 = info_fetch(sentence,died_set_lower,other_list_lower)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CARDINAL</th>\n",
       "      <th>GPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>Haikou</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CARDINAL     GPE\n",
       "0       15  Haikou"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_entity = other_entity_fetch(sentence)\n",
    "df2 = pd.DataFrame.from_dict(other_entity,orient='index')\n",
    "df2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              forced                                                                                                  \n",
      "  ______________________________________________________________________________________________|_______________                                                                                       \n",
      " |    |    |   |                      |                                       |                               caused                                                                                  \n",
      " |    |    |   |                      |                                       |                        _________|____________________________                                                          \n",
      " |    |    |   |                      |                                       |                       |   |              |                claimed                                                     \n",
      " |    |    |   |                      |                                       |                       |   |              |                   |                                                         \n",
      " |    |    |   |                      |                                       |                       |   |              |               inflorida                                                    \n",
      " |    |    |   |                      |                                       |                       |   |              |             ______|____________                                             \n",
      " |    |    |   |                    during                               displacement                 |   |              |            |      |         georgia                                        \n",
      " |    |    |   |                      |                                _______|__________             |   |              |            |      |       _____|_____________                               \n",
      " |    |    |   |                   sojourn                            |                  of           |   |           billion         |      |      |              pennsylvania                       \n",
      " |    |    |   |    __________________|__________________________     |                  |            |   |      ________|______      |      |      |      _____________|________                      \n",
      " |    |    |   |   |   |        |                 |              in   |                people         |   |     |               in    |      |      |     |                     york                  \n",
      " |    |    |   |   |   |        |                 |              |    |                  |            |   |     |               |     |      |      |     |      ________________|______               \n",
      " |    |    |   |   |   |       day             seaboard         june  |               100,000         |   |     |             damage  |    lives    |     |     |       |            virginia         \n",
      " |    |    |   |   |   |    ____|_____       _____|________      |    |        __________|______      |   |     |               |     |      |      |     |     |       |         ______|________      \n",
      " ,  agnes  ,   .   a   up five        -    the          eastern 1972 the     more              than   ,  and nearly$2         flood   ,     134     ,     ,    new      ,        ,     and    maryland\n",
      "\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "fetch_cardinal = {}\n",
    "sentence = sentence.lower()\n",
    "doc = nlp(sentence)\n",
    "tree = [to_nltk_tree(sent.root) for sent in doc.sents]\n",
    "print([to_nltk_tree(sent.root).pretty_print() for sent in doc.sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = fetch_graph(sentence)\n",
    "context_dict = fetch_label(sentence)\n",
    "spath_val_output_diedset = shortestpath_text(graph,sentence,died_set_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = length_shortest_path(spath_val_output_diedset)\n",
    "spath_val_output_otherset = shortestpath_text(graph,sentence,other_list_lower)\n",
    "l2 = length_shortest_path(spath_val_output_otherset)\n",
    "final_list = l1+l2\n",
    "df = pd.DataFrame(final_list)\n",
    "info = df.loc[df.groupby(1)[2].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['displacement']"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availablelist = word_availability_check(sentence,other_list_lower)\n",
    "availablelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'500': 'CARDINAL', 'Category 2': 'PRODUCT', 'three': 'CARDINAL'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def cardinal_check(dict1,dict2):\n",
    "# #     card_dict_dict = {}\n",
    "# #     cardinal_word_dict = {}\n",
    "# #     final_dict ={}\n",
    "\n",
    "# #     for (key1,value1) in dict1.items():\n",
    "# #         for (word,value2) in value1.items():\n",
    "# #             for (key2,value3) in dict2.items():\n",
    "# #                 if word == key2 and value3 == 'CARDINAL':\n",
    "# # #                     print('---', word, value2)\n",
    "# #                     cardinal_word_dict.update({word:value2})\n",
    "# #                 else:\n",
    "# #                     pass\n",
    "# #             print('--  ---', cardinal_word_dict)\n",
    "# #             card_dict_dict.update({key1:cardinal_word_dict})\n",
    "# # #             cardinal_word_dict.clear()\n",
    "                \n",
    "        \n",
    "                \n",
    "            \n",
    "# #     return dict1\n",
    "# # def cardinal_check(dict1,dict2):\n",
    "# #     card_dict_dict = {}\n",
    "# #     cardinal_word_dict = {}\n",
    "# #     final_dict ={}\n",
    "\n",
    "# #     for (key1,value1) in dict1.items():\n",
    "# #         for (word,value2) in value1.items():\n",
    "# #             for (key2,value3) in dict2.items():\n",
    "# #                 if word == key2 and value3 != 'CARDINAL':\n",
    "# # #                     print('---', word, value2)\n",
    "# #                     value1.pop(word)\n",
    "# #                 else:\n",
    "# #                     pass\n",
    "# #         card_dict_dict.update({key1:value1})\n",
    "                \n",
    "                \n",
    "        \n",
    "                \n",
    "            \n",
    "# #     return card_dict_dict\n",
    "# # newDict = dict(filter(lambda elem: {elem[0],elem[1]} if word == key2 and value3 == 'CARDINAL', value1.items() for value1))\n",
    "# # cardinal_dict = dict(filter(lambda elem:(elem[0],elem[1]) if (elem[0] == key2 and value3 == 'CARDINAL' else None, value1.items()))\n",
    "# #     for k, v in dict1.items():\n",
    "# #         for k_, v_ in v.items():\n",
    "# #             if v_ in dict2.keys() and dict2[v_] == 'CARDINAL':\n",
    "# #                 final_dict[k_] = \n",
    "# #                 continue\n",
    "# #             else:\n",
    "# #                 del dict1[k][k_]\n",
    "\n",
    "# # def get_closest_value(dict1,dict2):\n",
    "# #     for k1,v1 in dict1.items():\n",
    "# #         for k2,v2 in dict2.items():\n",
    "# #             if v1>v2:\n",
    "# #                 dict1.pop(k1)\n",
    "# #             elif v2<v1:\n",
    "# #                 dict2.pop(k2)\n",
    "                \n",
    "# # # word_val = []\n",
    "# # # for l in final_list:\n",
    "# # #     print(l)\n",
    "# # #     for x in final_list:\n",
    "# # #         print(x)\n",
    "# # #         if l[1] == x[1] and x[2]>l[2]:\n",
    "# # #             print(l)\n",
    "# # minValue = [i[0] for i in final_list if i[1] == min(x[1] for x in final_list)]\n",
    "# # # min(final_list, key=lambda t: (t[0],t[1],t[2]))\n",
    "# # minValue     \n",
    "\n",
    "# def get_closest_path(searchlist,otherlist):\n",
    "#     path_val =[]\n",
    "#     for l in searchlist:\n",
    "#         print(l)\n",
    "#         if len(l)==2:\n",
    "#             path_val.append((l[0],l[1],2))\n",
    "#             print(path_val)\n",
    "# #         elif len(l)>2:\n",
    "# #             for word in otherlist:\n",
    "# #                 print(word)\n",
    "# #                 if word in l:\n",
    "# #                     windex = l.index(word)\n",
    "# #                     my_list = l[windex:]\n",
    "# #                     print(my_list)\n",
    "# #                     if len(my_list)==2:\n",
    "# #                         path_val.append((my_list[0],my_list[1],2))\n",
    "# #                     else:\n",
    "# #                         path_val.append((my_list[0],my_list[1],len(my_list)))\n",
    "#         else:\n",
    "#             pass\n",
    "#         return path_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
